{"cells":[{"cell_type":"markdown","source":["## (1)BASE With corrected sentiments "],"metadata":{"id":"Ysu_uG-o0I-l"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Load the CSV file into a pandas DataFrame\n","df = pd.read_csv('tweet_emotions.csv', header=None, names=['id', 'sentiment', 'content'])\n","\n","# Keep only the specified sentiment labels\n","allowed_sentiments = ['neutral', 'hate', 'happiness', 'sadness', 'worry', 'love']\n","df = df[df['sentiment'].isin(allowed_sentiments)]\n","\n","# Reset the index after filtering the DataFrame\n","df.reset_index(drop=True, inplace=True)\n","\n","# Preprocess the tweet content by removing special characters, stop words, and converting to lowercase\n","stop_words = set(stopwords.words('english'))\n","def preprocess_text(text):\n","    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n","    text = text.lower()\n","    text = ' '.join([word for word in text.split() if word not in stop_words])\n","    return text\n","df['content'] = df['content'].apply(preprocess_text)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(df['content'], df['sentiment'], test_size=0.2, random_state=42)\n","\n","# Print the number of training and testing samples\n","print('Number of training samples:', len(X_train))\n","print('Number of testing samples:', len(X_test))\n","\n","# Convert the tweet content into a bag-of-words representation\n","vectorizer = CountVectorizer()\n","X_train = vectorizer.fit_transform(X_train)\n","X_test = vectorizer.transform(X_test)\n","\n","# Train a Naive Bayes classifier on the training set\n","classifier = MultinomialNB()\n","classifier.fit(X_train, y_train)\n","\n","# Predict the sentiment labels of the tweets in the testing set\n","y_pred_mapped = classifier.predict(X_test)\n","\n","# Evaluate the performance of the classifier\n","accuracy = accuracy_score(y_test, y_pred_mapped)\n","print('Accuracy:', accuracy)\n","print('Classification report:\\n', classification_report(y_test, y_pred_mapped))\n","\n","# Create a DataFrame with the testing data and predicted emotion\n","# Add the predicted sentiment to the original DataFrame\n","df_test = pd.DataFrame({'content': df.iloc[y_test.index]['content'].values, 'predicted_sentiment': y_pred_mapped})\n","df_test['id'] = df.iloc[y_test.index]['id'].values\n","df_test['sentiment'] = df.iloc[y_test.index]['sentiment'].values\n","\n","# Save the predicted sentiment DataFrame to a CSV file\n","df_test.to_csv('comparisonNaive.csv', index=False, columns=['id', 'sentiment', 'content', 'predicted_sentiment'])\n","\n","# Print the number of correctly classified samples\n","num_correct = (y_test == y_pred_mapped).sum()\n","print('Number of correctly classified samples:', num_correct)\n"],"metadata":{"id":"c9eGBkQJtdR2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681359112613,"user_tz":360,"elapsed":1918,"user":{"displayName":"jiro go","userId":"05602148120618578642"}},"outputId":"1e98c3f0-46ed-43e2-d616-d6f89fce08f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Number of training samples: 26108\n","Number of testing samples: 6528\n","Accuracy: 0.3866421568627451\n","Classification report:\n","               precision    recall  f1-score   support\n","\n","   happiness       0.45      0.34      0.38      1061\n","        hate       0.29      0.01      0.02       254\n","        love       0.55      0.28      0.37       785\n","     neutral       0.41      0.36      0.38      1708\n","     sadness       0.35      0.13      0.19      1007\n","       worry       0.35      0.70      0.46      1713\n","\n","    accuracy                           0.39      6528\n","   macro avg       0.40      0.30      0.30      6528\n","weighted avg       0.40      0.39      0.36      6528\n","\n","Number of correctly classified samples: 2524\n"]}]},{"cell_type":"markdown","source":["## (2)Stemming with Corrected sentiments "],"metadata":{"id":"dRMO0ofh01Sv"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report\n","from nltk.stem import PorterStemmer\n","\n","# Initialize the PorterStemmer class\n","stemmer = PorterStemmer()\n","\n","\n","# Load the CSV file into a pandas DataFrame\n","df = pd.read_csv('tweet_emotions.csv', header=None, names=['id', 'sentiment', 'content'])\n","\n","# Keep only the specified sentiment labels\n","allowed_sentiments = ['neutral', 'hate', 'happiness', 'sadness', 'worry', 'love']\n","df = df[df['sentiment'].isin(allowed_sentiments)]\n","\n","# Reset the index after filtering the DataFrame\n","df.reset_index(drop=True, inplace=True)\n","\n","# Preprocess the tweet content by removing special characters, stop words, and converting to lowercase\n","stop_words = set(stopwords.words('english'))\n","def preprocess_text(text):\n","    # Remove special characters\n","    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n","    \n","    # Convert to lowercase\n","    text = text.lower()\n","    \n","    # Remove stop words and perform stemming\n","    words = [stemmer.stem(word) for word in text.split() if word not in stop_words]\n","    text = ' '.join(words)\n","    \n","    return text\n","\n","df['content'] = df['content'].apply(preprocess_text)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(df['content'], df['sentiment'], test_size=0.2, random_state=42)\n","\n","# Print the number of training and testing samples\n","print('Number of training samples:', len(X_train))\n","print('Number of testing samples:', len(X_test))\n","\n","# Convert the tweet content into a bag-of-words representation\n","vectorizer = CountVectorizer()\n","X_train = vectorizer.fit_transform(X_train)\n","X_test = vectorizer.transform(X_test)\n","\n","# Train a Naive Bayes classifier on the training set\n","classifier = MultinomialNB()\n","classifier.fit(X_train, y_train)\n","\n","# Predict the sentiment labels of the tweets in the testing set\n","y_pred_mapped = classifier.predict(X_test)\n","\n","# Evaluate the performance of the classifier\n","accuracy = accuracy_score(y_test, y_pred_mapped)\n","print('Accuracy:', accuracy)\n","print('Classification report:\\n', classification_report(y_test, y_pred_mapped))\n","\n","# Create a DataFrame with the testing data and predicted emotion\n","# Add the predicted sentiment to the original DataFrame\n","df_test = pd.DataFrame({'content': df.iloc[y_test.index]['content'].values, 'predicted_sentiment': y_pred_mapped})\n","df_test['id'] = df.iloc[y_test.index]['id'].values\n","df_test['sentiment'] = df.iloc[y_test.index]['sentiment'].values\n","\n","# Save the predicted sentiment DataFrame to a CSV file\n","df_test.to_csv('comparisonNaive.csv', index=False, columns=['id', 'sentiment', 'content', 'predicted_sentiment'])\n","\n","# Print the number of correctly classified samples\n","num_correct = (y_test == y_pred_mapped).sum()\n","print('Number of correctly classified samples:', num_correct)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qynUxrdC0vaU","executionInfo":{"status":"ok","timestamp":1681359370650,"user_tz":360,"elapsed":8948,"user":{"displayName":"jiro go","userId":"05602148120618578642"}},"outputId":"a6525555-3d7c-47cd-fb37-f791b6f2ddbf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Number of training samples: 26108\n","Number of testing samples: 6528\n","Accuracy: 0.3874080882352941\n","Classification report:\n","               precision    recall  f1-score   support\n","\n","   happiness       0.44      0.33      0.38      1061\n","        hate       0.33      0.01      0.02       254\n","        love       0.56      0.28      0.37       785\n","     neutral       0.40      0.37      0.39      1708\n","     sadness       0.37      0.14      0.20      1007\n","       worry       0.35      0.69      0.46      1713\n","\n","    accuracy                           0.39      6528\n","   macro avg       0.41      0.30      0.30      6528\n","weighted avg       0.41      0.39      0.36      6528\n","\n","Number of correctly classified samples: 2529\n"]}]},{"cell_type":"markdown","source":["## (3) Negation with corrected sentiments "],"metadata":{"id":"PzYdrGBm1uVX"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report\n","from nltk.stem import PorterStemmer\n","\n","# Initialize the PorterStemmer class\n","stemmer = PorterStemmer()\n","\n","\n","# Load the CSV file into a pandas DataFrame\n","df = pd.read_csv('tweet_emotions.csv', header=None, names=['id', 'sentiment', 'content'])\n","# Keep only the specified sentiment labels\n","allowed_sentiments = ['neutral', 'hate', 'happiness', 'sadness', 'worry', 'love']\n","df = df[df['sentiment'].isin(allowed_sentiments)]\n","\n","# Reset the index after filtering the DataFrame\n","df.reset_index(drop=True, inplace=True)\n","# Preprocess the tweet content by removing special characters, stop words, and converting to lowercase\n","stop_words = set(stopwords.words('english'))\n","def preprocess_text(text):\n","    # Remove special characters\n","    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n","    \n","    # Convert to lowercase\n","    text = text.lower()\n","    \n","    # Remove stop words and perform stemming handling negations\n","    words = text.split()\n","    processed_words = []\n","    negate = False\n","    for i in range(len(words)):\n","        word = words[i]\n","        if word in stop_words:\n","            continue\n","        if negate:\n","            processed_words.append('NOT_' + stemmer.stem(word))\n","            negate = False\n","        elif i > 0 and words[i-1] in ['not', \"n't\", 'no', 'never']:\n","            processed_words.append('NOT_' + stemmer.stem(word))\n","        else:\n","            processed_words.append(stemmer.stem(word))\n","        if word in ['not', \"n't\", 'no', 'never']:\n","            negate = True\n","    \n","    text = ' '.join(processed_words)\n","    \n","    return text\n","\n","df['content'] = df['content'].apply(preprocess_text)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(df['content'], df['sentiment'], test_size=0.2, random_state=42)\n","\n","# Print the number of training and testing samples\n","print('Number of training samples:', len(X_train))\n","print('Number of testing samples:', len(X_test))\n","\n","# Convert the tweet content into a bag-of-words representation\n","vectorizer = CountVectorizer()\n","X_train = vectorizer.fit_transform(X_train)\n","X_test = vectorizer.transform(X_test)\n","\n","# Train a Naive Bayes classifier on the training set\n","classifier = MultinomialNB()\n","classifier.fit(X_train, y_train)\n","\n","# Predict the sentiment labels of the tweets in the testing set\n","y_pred_mapped = classifier.predict(X_test)\n","\n","# Evaluate the performance of the classifier\n","accuracy = accuracy_score(y_test, y_pred_mapped)\n","print('Accuracy:', accuracy)\n","print('Classification report:\\n', classification_report(y_test, y_pred_mapped))\n","\n","# Create a DataFrame with the testing data and predicted emotion\n","# Add the predicted sentiment to the original DataFrame\n","df_test = pd.DataFrame({'content': df.iloc[y_test.index]['content'].values, 'predicted_sentiment': y_pred_mapped})\n","df_test['id'] = df.iloc[y_test.index]['id'].values\n","df_test['sentiment'] = df.iloc[y_test.index]['sentiment'].values\n","\n","# Save the predicted sentiment DataFrame to a CSV file\n","df_test.to_csv('comparisonNaive.csv', index=False, columns=['id', 'sentiment', 'content', 'predicted_sentiment'])\n","\n","# Print the number of correctly classified samples\n","num_correct = (y_test == y_pred_mapped).sum()\n","print('Number of correctly classified samples:', num_correct)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffzuWEWk1Xbe","executionInfo":{"status":"ok","timestamp":1681359568224,"user_tz":360,"elapsed":15336,"user":{"displayName":"jiro go","userId":"05602148120618578642"}},"outputId":"8a1aa2eb-63ad-4f9e-d9b4-9e0f71767025"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Number of training samples: 26108\n","Number of testing samples: 6528\n","Accuracy: 0.3874080882352941\n","Classification report:\n","               precision    recall  f1-score   support\n","\n","   happiness       0.45      0.34      0.39      1061\n","        hate       0.33      0.01      0.02       254\n","        love       0.55      0.28      0.37       785\n","     neutral       0.41      0.37      0.39      1708\n","     sadness       0.36      0.13      0.20      1007\n","       worry       0.35      0.69      0.46      1713\n","\n","    accuracy                           0.39      6528\n","   macro avg       0.41      0.30      0.30      6528\n","weighted avg       0.41      0.39      0.36      6528\n","\n","Number of correctly classified samples: 2529\n"]}]},{"cell_type":"markdown","source":["## (4) Removing mentions and Hastags with corrected sentiments "],"metadata":{"id":"akL2weHh17aQ"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report\n","from nltk.stem import PorterStemmer\n","\n","# Initialize the PorterStemmer class\n","stemmer = PorterStemmer()\n","\n","\n","# Load the CSV file into a pandas DataFrame\n","df = pd.read_csv('tweet_emotions.csv', header=None, names=['id', 'sentiment', 'content'])\n","\n","# Keep only the specified sentiment labels\n","allowed_sentiments = ['neutral', 'hate', 'happiness', 'sadness', 'worry', 'love']\n","df = df[df['sentiment'].isin(allowed_sentiments)]\n","\n","# Reset the index after filtering the DataFrame\n","df.reset_index(drop=True, inplace=True)\n","\n","# Preprocess the tweet content by removing special characters, stop words, and converting to lowercase\n","stop_words = set(stopwords.words('english'))\n","def preprocess_text(text):\n","    # Remove special characters\n","    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n","    \n","     # Remove mentions (words starting with @)\n","    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","    \n","    # Remove hashtags (words starting with #)\n","    text = re.sub(r'#[A-Za-z0-9]+', '', text)\n","\n","    # Convert to lowercase\n","    text = text.lower()\n","    \n","    # Remove stop words and perform stemming\n","    words = text.split()\n","    processed_words = []\n","    negate = False\n","    for i in range(len(words)):\n","        word = words[i]\n","        if word in stop_words:\n","            continue\n","        if negate:\n","            processed_words.append('NOT_' + stemmer.stem(word))\n","            negate = False\n","        elif i > 0 and words[i-1] in ['not', \"n't\", 'no', 'never']:\n","            processed_words.append('NOT_' + stemmer.stem(word))\n","        else:\n","            processed_words.append(stemmer.stem(word))\n","        if word in ['not', \"n't\", 'no', 'never']:\n","            negate = True\n","    \n","    text = ' '.join(processed_words)\n","    \n","    return text\n","\n","df['content'] = df['content'].apply(preprocess_text)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(df['content'], df['sentiment'], test_size=0.2, random_state=42)\n","\n","# Print the number of training and testing samples\n","print('Number of training samples:', len(X_train))\n","print('Number of testing samples:', len(X_test))\n","\n","# Convert the tweet content into a bag-of-words representation\n","vectorizer = CountVectorizer()\n","X_train = vectorizer.fit_transform(X_train)\n","X_test = vectorizer.transform(X_test)\n","\n","# Train a Naive Bayes classifier on the training set\n","classifier = MultinomialNB()\n","classifier.fit(X_train, y_train)\n","\n","# Predict the sentiment labels of the tweets in the testing set\n","y_pred_mapped = classifier.predict(X_test)\n","\n","# Evaluate the performance of the classifier\n","accuracy = accuracy_score(y_test, y_pred_mapped)\n","print('Accuracy:', accuracy)\n","print('Classification report:\\n', classification_report(y_test, y_pred_mapped))\n","\n","# Create a DataFrame with the testing data and predicted emotion\n","# Add the predicted sentiment to the original DataFrame\n","df_test = pd.DataFrame({'content': df.iloc[y_test.index]['content'].values, 'predicted_sentiment': y_pred_mapped})\n","df_test['id'] = df.iloc[y_test.index]['id'].values\n","df_test['sentiment'] = df.iloc[y_test.index]['sentiment'].values\n","\n","# Save the predicted sentiment DataFrame to a CSV file\n","df_test.to_csv('comparisonNaive.csv', index=False, columns=['id', 'sentiment', 'content', 'predicted_sentiment'])\n","\n","# Print the number of correctly classified samples\n","num_correct = (y_test == y_pred_mapped).sum()\n","print('Number of correctly classified samples:', num_correct)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SJBDmjjO16Zq","executionInfo":{"status":"ok","timestamp":1681359688953,"user_tz":360,"elapsed":12480,"user":{"displayName":"jiro go","userId":"05602148120618578642"}},"outputId":"8ec9ea8b-67a3-4a7f-cd70-fb73909aae7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Number of training samples: 26108\n","Number of testing samples: 6528\n","Accuracy: 0.3874080882352941\n","Classification report:\n","               precision    recall  f1-score   support\n","\n","   happiness       0.45      0.34      0.39      1061\n","        hate       0.33      0.01      0.02       254\n","        love       0.55      0.28      0.37       785\n","     neutral       0.41      0.37      0.39      1708\n","     sadness       0.36      0.13      0.20      1007\n","       worry       0.35      0.69      0.46      1713\n","\n","    accuracy                           0.39      6528\n","   macro avg       0.41      0.30      0.30      6528\n","weighted avg       0.41      0.39      0.36      6528\n","\n","Number of correctly classified samples: 2529\n"]}]},{"cell_type":"markdown","source":["## (5) Ada boosting with corrected sentiments "],"metadata":{"id":"joViun4S2RVC"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report\n","from nltk.stem import PorterStemmer\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","# Initialize the PorterStemmer class\n","stemmer = PorterStemmer()\n","\n","\n","# Load the CSV file into a pandas DataFrame\n","df = pd.read_csv('tweet_emotions.csv', header=None, names=['id', 'sentiment', 'content'])\n","\n","# Keep only the specified sentiment labels\n","allowed_sentiments = ['neutral', 'hate', 'happiness', 'sadness', 'worry', 'love']\n","df = df[df['sentiment'].isin(allowed_sentiments)]\n","\n","# Reset the index after filtering the DataFrame\n","df.reset_index(drop=True, inplace=True)\n","\n","# Preprocess the tweet content by removing special characters, stop words, and converting to lowercase\n","stop_words = set(stopwords.words('english'))\n","def preprocess_text(text):\n","    # Remove special characters\n","    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n","    \n","     # Remove mentions (words starting with @)\n","    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","    \n","    # Remove hashtags (words starting with #)\n","    text = re.sub(r'#[A-Za-z0-9]+', '', text)\n","\n","    # Convert to lowercase\n","    text = text.lower()\n","    \n","    # Remove stop words and perform stemming\n","    words = text.split()\n","    processed_words = []\n","    negate = False\n","    for i in range(len(words)):\n","        word = words[i]\n","        if word in stop_words:\n","            continue\n","        if negate:\n","            processed_words.append('NOT_' + stemmer.stem(word))\n","            negate = False\n","        elif i > 0 and words[i-1] in ['not', \"n't\", 'no', 'never']:\n","            processed_words.append('NOT_' + stemmer.stem(word))\n","        else:\n","            processed_words.append(stemmer.stem(word))\n","        if word in ['not', \"n't\", 'no', 'never']:\n","            negate = True\n","    \n","    text = ' '.join(processed_words)\n","    \n","    return text\n","\n","df['content'] = df['content'].apply(preprocess_text)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(df['content'], df['sentiment'], test_size=0.2, random_state=42)\n","\n","# Print the number of training and testing samples\n","print('Number of training samples:', len(X_train))\n","print('Number of testing samples:', len(X_test))\n","\n","# Convert the tweet content into a bag-of-words representation\n","vectorizer = CountVectorizer()\n","X_train = vectorizer.fit_transform(X_train)\n","X_test = vectorizer.transform(X_test)\n","\n","# Train a Naive Bayes classifier on the training set using AdaBoost\n","base_estimator = MultinomialNB()\n","classifier = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, random_state=42)\n","classifier.fit(X_train, y_train)\n","\n","# Predict the sentiment labels of the tweets in the testing set\n","y_pred_mapped = classifier.predict(X_test)\n","\n","# Evaluate the performance of the classifier\n","accuracy = accuracy_score(y_test, y_pred_mapped)\n","print('Accuracy:', accuracy)\n","print('Classification report:\\n', classification_report(y_test, y_pred_mapped))\n","\n","# Create a DataFrame with the testing data and predicted emotion\n","# Add the predicted sentiment to the original DataFrame\n","df_test = pd.DataFrame({'content': df.iloc[y_test.index]['content'].values, 'predicted_sentiment': y_pred_mapped})\n","df_test['id'] = df.iloc[y_test.index]['id'].values\n","df_test['sentiment'] = df.iloc[y_test.index]['sentiment'].values\n","\n","#Save the predicted sentiment DataFrame to a CSV file\n","df_test.to_csv('comparisonNaive.csv', index=False, columns=['id', 'sentiment', 'content', 'predicted_sentiment'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fxIC65E62Q6t","executionInfo":{"status":"ok","timestamp":1681359793607,"user_tz":360,"elapsed":23882,"user":{"displayName":"jiro go","userId":"05602148120618578642"}},"outputId":"2b61de78-d553-41b5-8dbb-3950f5d1e8cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Number of training samples: 26108\n","Number of testing samples: 6528\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.36580882352941174\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Classification report:\n","               precision    recall  f1-score   support\n","\n","   happiness       0.48      0.07      0.12      1061\n","        hate       0.00      0.00      0.00       254\n","        love       0.60      0.28      0.38       785\n","     neutral       0.34      0.74      0.46      1708\n","     sadness       0.56      0.01      0.03      1007\n","       worry       0.37      0.47      0.41      1713\n","\n","    accuracy                           0.37      6528\n","   macro avg       0.39      0.26      0.23      6528\n","weighted avg       0.42      0.37      0.30      6528\n","\n"]}]},{"cell_type":"markdown","source":["### (6) USing CNN insted of Naive with corrected sentiments "],"metadata":{"id":"ZJkD-Wuq6PjP"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import nltk\n","import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","\n","stemmer = PorterStemmer()\n","\n","stop_words = set(stopwords.words('english'))\n","def preprocess_text(text):\n","     # Remove special characters\n","    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n","    \n","     # Remove mentions (words starting with @)\n","    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","    \n","    # Remove hashtags (words starting with #)\n","    text = re.sub(r'#[A-Za-z0-9]+', '', text)\n","\n","    # Convert to lowercase\n","    text = text.lower()\n","    \n","    # Remove stop words and perform stemming\n","    words = text.split()\n","    processed_words = []\n","    negate = False\n","    for i in range(len(words)):\n","        word = words[i]\n","        if word in stop_words:\n","            continue\n","        if negate:\n","            processed_words.append('NOT_' + stemmer.stem(word))\n","            negate = False\n","        elif i > 0 and words[i-1] in ['not', \"n't\", 'no', 'never']:\n","            processed_words.append('NOT_' + stemmer.stem(word))\n","        else:\n","            processed_words.append(stemmer.stem(word))\n","        if word in ['not', \"n't\", 'no', 'never']:\n","            negate = True\n","    \n","    text = ' '.join(processed_words)\n","    \n","    return text\n","\n","df = pd.read_csv('tweet_emotions.csv', header=None, names=['id', 'sentiment', 'content'])\n","df['content'] = df['content'].apply(preprocess_text)\n","\n","# Keep only the specified sentiment labels\n","allowed_sentiments = ['neutral', 'hate', 'happiness', 'sadness', 'worry', 'love']\n","df = df[df['sentiment'].isin(allowed_sentiments)]\n","\n","# Reset the index after filtering the DataFrame\n","df.reset_index(drop=True, inplace=True)\n","\n","# Tokenize the text and pad the sequences\n","max_words = 10000\n","tokenizer = Tokenizer(num_words=max_words, lower=True)\n","tokenizer.fit_on_texts(df['content'])\n","\n","X = tokenizer.texts_to_sequences(df['content'])\n","X = pad_sequences(X, padding='post')\n","y = pd.get_dummies(df['sentiment']).values\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Build the CNN model\n","embedding_dim = 100\n","model = Sequential([\n","    Embedding(max_words, embedding_dim, input_length=X.shape[1]),\n","    Conv1D(128, 5, activation='relu'),\n","    GlobalMaxPooling1D(),\n","    Dense(30, activation='relu'),\n","    Dense(y.shape[1], activation='softmax')\n","])\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n","\n","# Train the CNN model\n","epochs = 5\n","batch_size = 32\n","model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n","\n","# Evaluate the performance\n","y_pred = model.predict(X_test)\n","y_pred_mapped = np.argmax(y_pred, axis=1)\n","y_test_mapped = np.argmax(y_test, axis=1)\n","\n","accuracy = accuracy_score(y_test_mapped, y_pred_mapped)\n","print('Accuracy:', accuracy)\n","print('Classification report:\\n', classification_report(y_test_mapped, y_pred_mapped))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ROBf05656MdI","executionInfo":{"status":"ok","timestamp":1681361109099,"user_tz":360,"elapsed":218993,"user":{"displayName":"jiro go","userId":"05602148120618578642"}},"outputId":"9a2fc22e-3916-4b51-be5b-dc3fe2060899"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 24, 100)           1000000   \n","                                                                 \n"," conv1d (Conv1D)             (None, 20, 128)           64128     \n","                                                                 \n"," global_max_pooling1d (Globa  (None, 128)              0         \n"," lMaxPooling1D)                                                  \n","                                                                 \n"," dense (Dense)               (None, 30)                3870      \n","                                                                 \n"," dense_1 (Dense)             (None, 6)                 186       \n","                                                                 \n","=================================================================\n","Total params: 1,068,184\n","Trainable params: 1,068,184\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/5\n","735/735 [==============================] - 35s 45ms/step - loss: 1.4820 - accuracy: 0.3975 - val_loss: 1.3747 - val_accuracy: 0.4458\n","Epoch 2/5\n","735/735 [==============================] - 29s 40ms/step - loss: 1.2236 - accuracy: 0.5189 - val_loss: 1.3873 - val_accuracy: 0.4362\n","Epoch 3/5\n","735/735 [==============================] - 26s 36ms/step - loss: 0.9016 - accuracy: 0.6702 - val_loss: 1.5968 - val_accuracy: 0.4079\n","Epoch 4/5\n","735/735 [==============================] - 28s 39ms/step - loss: 0.5485 - accuracy: 0.8133 - val_loss: 1.9263 - val_accuracy: 0.3903\n","Epoch 5/5\n","735/735 [==============================] - 30s 41ms/step - loss: 0.3268 - accuracy: 0.8953 - val_loss: 2.3572 - val_accuracy: 0.3704\n","204/204 [==============================] - 1s 5ms/step\n","Accuracy: 0.35891544117647056\n","Classification report:\n","               precision    recall  f1-score   support\n","\n","           0       0.36      0.35      0.35      1061\n","           1       0.24      0.23      0.23       254\n","           2       0.36      0.42      0.39       785\n","           3       0.40      0.38      0.39      1708\n","           4       0.28      0.30      0.29      1007\n","           5       0.38      0.38      0.38      1713\n","\n","    accuracy                           0.36      6528\n","   macro avg       0.34      0.34      0.34      6528\n","weighted avg       0.36      0.36      0.36      6528\n","\n"]}]},{"cell_type":"markdown","source":["## (7)Using a bigger database by connecting 2 databases  with corrected sentiments"],"metadata":{"id":"gaq0Kaa63HcF"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report\n","from nltk.stem import PorterStemmer\n","\n","# Initialize the PorterStemmer class\n","stemmer = PorterStemmer()\n","\n","\n","# Load the CSV file into a pandas DataFrame\n","df = pd.read_csv('combined_data.csv', header=None, names=['sentiment', 'content','Original Content'])\n","\n","# Keep only the specified sentiment labels\n","allowed_sentiments = ['neutral', 'hate', 'happiness', 'sadness', 'worry', 'love']\n","df = df[df['sentiment'].isin(allowed_sentiments)]\n","\n","# Reset the index after filtering the DataFrame\n","df.reset_index(drop=True, inplace=True)\n","\n","# Preprocess the tweet content by removing special characters, stop words, and converting to lowercase\n","stop_words = set(stopwords.words('english'))\n","def preprocess_text(text):\n","    # Remove special characters\n","    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n","    \n","     # Remove mentions (words starting with @)\n","    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","    \n","    # Remove hashtags (words starting with #)\n","    text = re.sub(r'#[A-Za-z0-9]+', '', text)\n","\n","    # Convert to lowercase\n","    text = text.lower()\n","    \n","    # Remove stop words and perform stemming\n","    words = text.split()\n","    processed_words = []\n","    negate = False\n","    for i in range(len(words)):\n","        word = words[i]\n","        if word in stop_words:\n","            continue\n","        if negate:\n","            processed_words.append('NOT_' + stemmer.stem(word))\n","            negate = False\n","        elif i > 0 and words[i-1] in ['not', \"n't\", 'no', 'never']:\n","            processed_words.append('NOT_' + stemmer.stem(word))\n","        else:\n","            processed_words.append(stemmer.stem(word))\n","        if word in ['not', \"n't\", 'no', 'never']:\n","            negate = True\n","    \n","    text = ' '.join(processed_words)\n","    \n","    return text\n","\n","df['content'] = df['content'].apply(preprocess_text)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(df['content'], df['sentiment'], test_size=0.2, random_state=42)\n","\n","# Print the number of training and testing samples\n","print('Number of training samples:', len(X_train))\n","print('Number of testing samples:', len(X_test))\n","\n","# Convert the tweet content into a bag-of-words representation\n","vectorizer = CountVectorizer()\n","X_train = vectorizer.fit_transform(X_train)\n","X_test = vectorizer.transform(X_test)\n","\n","# Train a Naive Bayes classifier on the training set\n","classifier = MultinomialNB()\n","classifier.fit(X_train, y_train)\n","\n","# Predict the sentiment labels of the tweets in the testing set\n","y_pred_mapped = classifier.predict(X_test)\n","\n","# Evaluate the performance of the classifier\n","accuracy = accuracy_score(y_test, y_pred_mapped)\n","print('Accuracy:', accuracy)\n","print('Classification report:\\n', classification_report(y_test, y_pred_mapped))\n","\n","# Create a DataFrame with the testing data and predicted emotion\n","# Add the predicted sentiment to the original DataFrame\n","df_test = pd.DataFrame({'content': df.iloc[y_test.index]['content'].values, 'predicted_sentiment': y_pred_mapped})\n","df_test['id'] = df.iloc[y_test.index]['id'].values\n","df_test['sentiment'] = df.iloc[y_test.index]['sentiment'].values\n","\n","# Save the predicted sentiment DataFrame to a CSV file\n","df_test.to_csv('comparisonNaive.csv', index=False, columns=['id', 'sentiment', 'content', 'predicted_sentiment'])\n","\n","# Print the number of correctly classified samples\n","num_correct = (y_test == y_pred_mapped).sum()\n","print('Number of correctly classified samples:', num_correct)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":831},"id":"c6zAI7Gu3GFY","executionInfo":{"status":"error","timestamp":1681360798379,"user_tz":360,"elapsed":247258,"user":{"displayName":"jiro go","userId":"05602148120618578642"}},"outputId":"68dcd939-dc7e-4469-92a6-2588e19a490a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Number of training samples: 759368\n","Number of testing samples: 189843\n","Accuracy: 0.8252819435006822\n","Classification report:\n","               precision    recall  f1-score   support\n","\n","   happiness       0.86      0.90      0.88     61399\n","        hate       0.86      0.76      0.81     60197\n","        love       0.65      0.01      0.03       753\n","     neutral       0.31      0.01      0.02      1735\n","     sadness       0.77      0.87      0.82     64013\n","       worry       0.29      0.01      0.02      1746\n","\n","    accuracy                           0.83    189843\n","   macro avg       0.62      0.43      0.43    189843\n","weighted avg       0.82      0.83      0.82    189843\n","\n"]},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'id'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-310296b6376d>\u001b[0m in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Add the predicted sentiment to the original DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predicted_sentiment'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_pred_mapped\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'id'"]}]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}